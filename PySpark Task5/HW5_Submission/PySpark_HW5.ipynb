{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gppE6GTPhNdl"
   },
   "source": [
    "# Setting UP PySpark enviroment in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KgOhUZTe1vZ"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark py4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoPVD2eUw2an"
   },
   "source": [
    "# Question 1 and Question 2\n",
    "\n",
    "Spark uses shared variables for parallel processing. A copy of shared variable goes on each node of the cluster when the driver sends a task to the executor on the cluster, so that it can be used for performing tasks.\n",
    "\n",
    "Two types of shared variables supported by Spark:\n",
    "- Broadcast\n",
    "- Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "mXQ0CdfaxZof"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1z4b4-AwyI4"
   },
   "source": [
    "## Broadcast\n",
    "\n",
    "Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, PySpark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs.  Broadcast variables are used in the same way for RDD and DataFrames and when we run these applications that have the broadcast variables defined and used, PySpark does the following:\n",
    "\n",
    "- PySpark breaks the job into stages that have distributed shuffling and actions are executed with in the stage.\n",
    "- Later Stages are also broken into tasks.\n",
    "- Spark broadcasts the common data (reusable) needed by tasks within each stage.\n",
    "- The broadcasted data is cache in serialized format and deserialized before executing each task.\n",
    "\n",
    "The following example comes from the Broadcast Variable source from the Reference tab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "jhzqPRk_xdbZ"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Broadcast App').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "rqoKK-T3y6L5"
   },
   "outputs": [],
   "source": [
    "# This function takes the broadcast variable to look for its corresponding state\n",
    "def state_convert(x):\n",
    "    return broadcast_states.value[x] # x value used to access values in broadcast_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ly5xlWAVx3Pg",
    "outputId": "a0bbbf6c-34bc-44c4-9d76-ee48c96197a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcast Variable results:  [('Carolina', 'Licona', 'USA', 'Tallahassee'), ('Vicente', 'De Leon', 'USA', 'Miami'), ('Max', 'Licona', 'PTY', 'Panama'), ('Maria', 'Jones', 'USA', 'Tallahassee')]\n"
     ]
    }
   ],
   "source": [
    "# Broadcast variables are used to save the copy of data across all nodes\n",
    "# Note that broadcast variables are not sent to executors with sc.broadcast(variable) call instead,\n",
    "# they will be sent to executors when they are first used.\n",
    "\n",
    "states = {'PA':'Panama', 'TLH':'Tallahassee', 'MIA':'Miami'} # dictionary\n",
    "\n",
    "lst1 = [('Carolina','Licona','USA','TLH'),\n",
    "        ('Vicente','De Leon','USA','MIA'),\n",
    "        ('Max','Licona','PTY','PA'),\n",
    "        ('Maria','Jones','USA','TLH')]\n",
    "\n",
    "broadcast_states = spark.sparkContext.broadcast(states)\n",
    "rdd = spark.sparkContext.parallelize(lst1) # using parallelize() function to create rdd from a list collection\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect() # extracts elements from tuples and applies state_convert function\n",
    "print('Broadcast Variable results: ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bk2Vl3VS3LC4"
   },
   "source": [
    "- PA -> Panama\n",
    "- TLH -> Tallahassee\n",
    "- MIA -> Miami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfRuCiwYhi0a"
   },
   "source": [
    "## Accumulator\n",
    "\n",
    "It's a shared variable concetp to aggregate information.\n",
    "\n",
    "It’s a shared variable that is used with RDD and DataFrame (distributed data) to perform sum and counter operations like Map_reduce counters. These variables are shared by all executors to update and add information through aggregation or computative operations.  They’re basically shared variables that can be updated by executors and propagate back to driver program for result collections. The following example comes from the “Accumulators” reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CD6AYS4MoT9r"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Accumulator App').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXmBhSyav_E-"
   },
   "source": [
    "Worker tasks on a Spark cluster can add values to an Accumulator with the += operator, but only the driver program is allowed to access its value, using value. Updates from the workers get propagated automatically to the driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "sPPa5lM6qXJq"
   },
   "outputs": [],
   "source": [
    "def accum_example(x): # accum_example function takes x(input) and updates the acc_sum by adding the x value to it.\n",
    "  global acc_sum\n",
    "  acc_sum += x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgE7N1BKo2Sf",
    "outputId": "d631b83e-950c-487e-a1b1-9dd508babbca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulator value: 15\n"
     ]
    }
   ],
   "source": [
    "# Accumulator variables are used for aggregating the information through associative and commutative operations\n",
    "# See pyspark.Accumulator documentation on how to use the spark.sparkContext.accumulator(0)\n",
    "# It also supports data types like int and float\n",
    "\n",
    "lst2 = [1, 2, 3, 4, 5]\n",
    "acc_sum = spark.sparkContext.accumulator(0) # the accumulator. It uses the accumulator method, initializing with 0.\n",
    "rdd = spark.sparkContext.parallelize(lst2) # using parallelize() function to create rdd from a list collection\n",
    "\n",
    "rdd.foreach(accum_example) # applies accum_example function to each element in the rdd using foreach\n",
    "accumulator_value = acc_sum.value\n",
    "print('Accumulator value:', accumulator_value) # returns acc sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzXRP73YsI6z"
   },
   "source": [
    "Other example that will also return 15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rO66b9yDodKa",
    "outputId": "e912d4db-e976-4b9a-cbac-6d2ad34c0dc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "accum=spark.sparkContext.accumulator(0)\n",
    "rdd=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOguym4B8pr_"
   },
   "source": [
    "## When to use?\n",
    "\n",
    "`Broadcast variables` are used to save the copy of data across all nodes. This variable is used, for example, to allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. It is suitable for resubale data that needs to be accessed by multiple tasks. An `accumulator` is used for aggregating the information through associative and commutative operations. Used for tasks like counting operations, sum operations, collecting computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0kDMAq2-gxs"
   },
   "source": [
    "# Question 3\n",
    "\n",
    "Why do we need UDF (User Defined Functions)?\n",
    "\n",
    "UDFs are used to extend the functions of the framework and re-use these functions on multiple DataFrames. This means we can create our functions to perform operations on dataframe columns and reuse these functions across multiple dataframes and SQL Expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "-V-19UdmGvYC"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "j_fmDnskEMIn"
   },
   "outputs": [],
   "source": [
    "# Function to return upper case letters. Applied to UDF new column\n",
    "def caps_lock(str):\n",
    "    return str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Cl-WqzROA1sJ"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('UDF App').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-ktdYR9Ltyg"
   },
   "source": [
    "Creating dataframe containing 2 columns: CustomerID and Customer Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sww6ygpWBBx1",
    "outputId": "c31676ed-c1c3-4b04-8118-e90206f97c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|CustomerID|Customer Name  |\n",
      "+----------+---------------+\n",
      "|123       |Vicente De Leon|\n",
      "|456       |Carolina Licona|\n",
      "|789       |Max Licona     |\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['CustomerID', 'Customer Name']\n",
    "\n",
    "data = [('123', 'Vicente De Leon'),\n",
    "        ('456', 'Carolina Licona'),\n",
    "        ('789', 'Max Licona')]\n",
    "\n",
    "df3 = spark.createDataFrame(data = data, schema = columns)\n",
    "df3.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqxaRSAdKDZw"
   },
   "source": [
    "Converting caps_lock() function to UDF and then use it with dataframe withColumn(). Example, creating the Caps Lock Name column to get the upper case lleter Customer Names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ncfBAId5Dj64",
    "outputId": "5bdd4bdc-9cd3-479f-a67f-54bc620d6853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------------------+\n",
      "|CustomerID|Customer Name  |Upper Case Customer Name|\n",
      "+----------+---------------+------------------------+\n",
      "|123       |Vicente De Leon|VICENTE DE LEON         |\n",
      "|456       |Carolina Licona|CAROLINA LICONA         |\n",
      "|789       |Max Licona     |MAX LICONA              |\n",
      "+----------+---------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "caps_lock_UDF = udf(lambda x: caps_lock(x)) # converting function to UDF, by default -> UDF StringType()\n",
    "# caps_lock_UDF = udf(lambda z: caps_lock(z), StringType())\n",
    "\n",
    "# Apply the caps_lock_UDF to create new column -> Upper Case Customer Name\n",
    "df4 = df3.withColumn('Upper Case Customer Name', caps_lock_UDF(col('Customer Name')))\n",
    "\n",
    "df4.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Oq_3tgnlW6w"
   },
   "source": [
    "# References:\n",
    "\n",
    "- Broadcast Variables: https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/\n",
    "\n",
    "- Accumulators: https://sparkbyexamples.com/pyspark/pyspark-accumulator-with-example/\n",
    "\n",
    "- Broadcast and Accumulator: https://data-flair.training/blogs/pyspark-broadcast-and-accumulator/\n",
    "\n",
    "- pyspark.Accumulator: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.Accumulator.html#:~:text=Accumulator,-class%20pyspark.&text=A%20shared%20variable%20that%20can,access%20its%20value%2C%20using%20value%20.\n",
    "\n",
    "- parallelize(): https://sparkbyexamples.com/pyspark/pyspark-parallelize-create-rdd/\n",
    "\n",
    "- tutorial: https://www.tutorialspoint.com/pyspark/pyspark_broadcast_and_accumulator.htm\n",
    "\n",
    "- tutorial: https://medium.com/@sangee01sankar17/broadcast-and-accumulator-variable-in-pyspark-5506dd32cae7\n",
    "\n",
    "- UDF: https://sparkbyexamples.com/pyspark/pyspark-udf-user-defined-function/\n",
    "\n",
    "- UDF: https://medium.com/@vaishalisubbaraj/user-defined-function-in-pyspark-e9740dc2d3bd\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
